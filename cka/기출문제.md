# 1. Retrieve Error Messages from a Container Log
- Cluster: kubectl config use-context hk8s
In the customera namespace, check the log for the app container in the cusom-app Pod. Save the line which contain the text "error" to the file /var/CKA2022/errors.txt

- application의 log를 출력 -> 추출해서 -> 파일에 저장
```
kubectl config use-context hk8s
kubectl config current-context

kubectl get pod -n customera
k logs custom-app -n customera | grep -i error > /var/CKA2022/errors.txt
cat /data/CKA/errors.txt
```

# 2. Node Troubleshooting
- Cluster: kubectl config use-context hk8s
A Kubernetes worker node, named `hk8s-worker2` is in state NotReady.
Investigate(조사하다) why this is the case, and perform any appropriate(적절한) steps to bring the node to a Ready state, ensuring that any change are made permanent

```
# 1. containerd 동작
# 2. kubelet 동작

kubectl get nodes
ssh hk8s-worker2

systemctl status containerd
systemctl status kubelet

대부분
systemctl enable --now kubelet
systemctl status kubelet

exit
kubectl get nodes
```

# 3. Count the Number of Nodes That Are Ready to Run Normal Workloads
- Cluster: kubectl config use-context hk8s
Determine how many nodes in cluster are ready to run normal workloads (i.e., workload that do not have any special tolerations). Output this number to the file `var/CKA2022/count.txt`.

마스터 노드는 Control plane으로만 사용한다. Taints를 확인해보면 control-plane이 나옴
하지만 다른 워커 노드들은 none으로 나온다. 근데 만약에 스케줄링을 마스터 노드도 포함시키고 싶으면 toleration을 설정하면 된다.

```
$ kubectl get nodes
ready 상태의 노드를 확인

$ kubectl describe nodes hk8s-worker1 | grep -i taint
Taints:         <none>
$ kubectl describe nodes hk8s-worker1 | grep -i taint
$ echo "2" > /var/CKA2022/count.txt
```

# 4. Management Node
- Cluster: kubectl config use-context hk8s
Set the node named `hk8s-worker1` as unavailable and reschedule all the pods running on it
```
$ kubectl config use-context hk8s
$ kubectl get pods -A 
// 모든 노드의 파드를 봄

# drain: node에서 동작중인 pod를 비우기. 모두삭제.
$ kubectl drain hk8s-worker1
```

reference -> kubectl reference docs

# ETCD Backup & Restore
etcd: 쿠버네티스의 정보를 저장하고 있는 곳, 쿠버네티스 cluster의 모든 info를 가지고 있음.
## etcd 백업
etcd는 메모리에 데이터를 가지고 있는데 이를 파일로 만들라는 문제다.
백업은 마스터노드에 있는 API에 요청해야한다. API에 요청하기 위해서는 인증정보가 필요하다.
이는 인증서를 사용하면 된다.

First, create a snapshot of the existing etcd instance running at `https://<k8s-master's IP 10.0.2.10>:2379`, saving the snapshot to `/data/etcd-snapshot.db`
Next, restore an existing, previous snapshot located at `/data/etcd-snapshot-previous.db`.
The following TLS certificates/key are supllied for connecting to the server with etcdctl:
- CA certificate: `/data/cka/ca.crt`
- Client certificate: `/data/cka/server.crt`
- Client key: `/data/cka/server.key`
- etcd backup 검색
```
$ etcd --version
$ ETCDCTL_API=3 etcdctl --endpoints=https://10.0.2.10:2379
--cacert=/data/cka/ca.crt --cert=/data/cka/server/crt --key=/data/cka/server.key snapshot save /data/etcd-snapshot.db

$ ls -l /data
```

- console에서 k8s master의 etcd backup 
```
### RESTORE
# Next, restore an existing, previous snapshot located at /data/etcd-snapshot-previous.db.

$ ssh k8s-master
$ su -i
$ ls /data/
etcd-snapshot-previous.sb

$ etcdutl --data-dir /var/lib/etcd-snapshot snapshot restore /data/etcd-snaphost.db

$ vi /etc/kubernetes/manifests/etcd.yaml

- hostPath:
  path: /var/libe/etcd-snapshot
  
2~3분 후에
kubectl get pods
```

# Cluster Upgrade - only Master
upgrade system : k8s-master
Given an exsiting Kubernetes clust running version `1.30.0`,
upgrade all of the Kubenetes control plane and node components on the master node only to version `1.30.6`. Be sure to `drain` the `master` node before upgrading it and `uncordon` it after the upgrade.

upgrade kubeadm

1. kubeadm upgrade

```
$ ssh k8s-master
$ sudo -i
$ apt update
$ apt-cache madison kubeadm
// 최신 버전 확인할 수 있음

$ sudo apt-mark unhold kubeadm
$ sudo apt-get update
$ sudo apt-get install -y kubeadm='1.30.6-1.1'
$ sudo apt-mark hold kubeadm

$ kubeadm version

## component들을 업그레이드 할 수 있다.
$ sudo kubeadm upgrade plan 

$ sudo kubeadm upgrade apply v1.30.6 -y

## drain을 통해 비우기 명령을 해야 함.
$ kubectl drain k8s-master --ignore-daemonsets

$ sudo apt-mark unhold kubelet kubectl
$ sudo apt-get update
$ sudo apt-get install -y kubelet='1.30.6'
$ sudo apt-mark hold kubelet kubectl

$ sudo systemctl daemon-reload
$ sudo systemctl restart kubelet

$ kubectl uncordon k8s-master

$ kubectl get nodes
```

# 7. Authentication and Authorization
- Cluster: k8s
## Context
You have been asked to create a new ClusterRole for a deployment pipeline and bind it to a specific ServiceAccount scoped to a specific namespace.
## Task:
- Create a new ClusterRole named `deployment-clusterrole`, which only allows to creawte the following resource types: Deployment StatefulSet DaemonSet
- Create a new ServiceAccount named `cicd-token` in the existing namespace app-team1
- Bind the new ClusterRole deployment-clusterrole to the new ServiceAccount cicd-token, limited to namespace app-team1
## 개념
- API, 모니터링 pod 툴, 실제 Pod가 있다고 가정하자
- 모니터링 pod 툴은 각 Pod를 볼 수 있는 권한, Node를 볼 수 있는 권한 등등이 있어야 한다.
- 권한을 ServiceAccount로 만들어서 해당 모니터링 Pod 툴에 Binding 할 수 있어야 한다.
## 풀이법
- RBAC 검색
- command-line utilities
- ServiceAccount

```
$ kubectl create sa cicd-token -n app-teamp1
$ kubectl get sa -n app-team1
$ kubectl create clusterrole deployment-clusterrole --verb=create --resource=deployments,statefulset,daemonset
$ kubectl create clusterrolebinding test-binding --clusterrole=cluster-admin --serviceaccount=app-team1:cicd-token
```

# 8. Pod 생성하기
- Cluster : kubectl config use-context k8s
## Context
Create a new namespace and create a pod in the namespace
## Task
- namespace name: cka-exam
- pod name: pod-01
- image: busybox
- environment Variable: CERT = "CKA-cert"
- command: /bin/sh
- arg = -c "while true; do echo $(CERT); sleep 10; done"
## 답
```
$ kubectl create namespace cka-exam
$ kubectl get namespaces cka-exam

$ kubectl run pod-01 --image=busybox --env CERT='CKA-cert' --command /bin/sh --dry-run=client -o yaml > pod-01.yaml
$ vi pod-01.yaml

```